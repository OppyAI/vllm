{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f810bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 12:01:26 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 08-11 12:01:28 [config.py:840] This model supports multiple tasks: {'classify', 'generate', 'reward', 'score', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 08-11 12:01:29 [config.py:1454] Using max model len 1024\n",
      "WARNING 08-11 12:01:29 [arg_utils.py:1724] --enable-prompt-embeds is not supported by the V1 Engine. Falling back to V0. \n",
      "WARNING 08-11 12:01:29 [cuda.py:102] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 08-11 12:01:29 [llm_engine.py:230] Initializing a V0 LLM engine (v0.1.dev7407+gae88822.d20250716) with config: model='/project/phan/kt477/OppyAI_backend/qwen7b_enc_dec_clean_no_att_on_client_dec', speculative_config=None, tokenizer='Qwen/Qwen2.5-Coder-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/project/phan/kt477/OppyAI_backend/qwen7b_enc_dec_clean_no_att_on_client_dec, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 08-11 12:01:29 [cuda.py:347] Using Flash Attention backend.\n",
      "INFO 08-11 12:01:30 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 08-11 12:01:30 [model_runner.py:1172] Starting to load model /project/phan/kt477/OppyAI_backend/qwen7b_enc_dec_clean_no_att_on_client_dec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a518148191c24bb4910f81e12ad16971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 12:01:31 [default_loader.py:272] Loading weights took 0.80 seconds\n",
      "INFO 08-11 12:01:31 [model_runner.py:1204] Model loading took 2.4751 GiB and 0.854045 seconds\n",
      "Positions: tensor([0, 1, 2,  ..., 5, 6, 7], device='cuda:0')\n",
      "Layer: DummyDecoderLayer()\n",
      "Error in RMSNorm: too many values to unpack (expected 2). Skipping RMSNorm.\n",
      "INFO 08-11 12:01:32 [worker.py:304] Memory profiling takes 0.40 seconds\n",
      "INFO 08-11 12:01:32 [worker.py:304] the current vLLM instance can use total_gpu_memory (79.32GiB) x gpu_memory_utilization (0.20) = 15.86GiB\n",
      "INFO 08-11 12:01:32 [worker.py:304] model weights take 2.48GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is 11.89GiB.\n",
      "INFO 08-11 12:01:32 [executor_base.py:113] # cuda blocks: 13920, # CPU blocks: 4681\n",
      "INFO 08-11 12:01:32 [executor_base.py:118] Maximum concurrency for 1024 tokens per request: 217.50x\n",
      "INFO 08-11 12:01:34 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 2.56 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizer, PretrainedConfig, AutoConfig, AutoModel\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from typing import Callable, List, Optional, Tuple, Union, Dict\n",
    "from torch import nn\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n",
    "from transformers.cache_utils import Cache\n",
    "from vllm import LLM\n",
    "from vllm import SamplingParams\n",
    "\n",
    "\n",
    "def register():\n",
    "    from vllm import ModelRegistry\n",
    "    from decoder import XCodeDecForCausalLM, XCodeDecConfig  # Import decoder classes\n",
    "\n",
    "    AutoConfig.register(\"xcodedec\", XCodeDecConfig)  # Register decoder config\n",
    "    ModelRegistry.register_model(\"XCodeDecModelForCausalLM\", XCodeDecForCausalLM)  # Register decoder model\n",
    "    from middle_model import XCodeForCausalLM, XCodeMiddleConfig  # Changed to absolute import\n",
    "\n",
    "    AutoConfig.register(\"xcodemiddle\", XCodeMiddleConfig)\n",
    "    ModelRegistry.register_model(\"XCodeMiddleModelForCausalLM\", XCodeForCausalLM)\n",
    "\n",
    "    from encoder import XCodeEncForCausalLM, XCodeEncConfig  # Import encoder classes\n",
    "\n",
    "    AutoConfig.register(\"xcodeenc\", XCodeEncConfig)  # Register encoder config\n",
    "    ModelRegistry.register_model(\"XCodeEncModelForCausalLM\", XCodeEncForCausalLM)  # Register encoder model\n",
    "\n",
    "    from enc_dec import XCodeEncDecConfig, XCodeEncDecForCausalLM  # Import encoder classes\n",
    "\n",
    "    AutoConfig.register(\"xcodeencdec\", XCodeEncDecConfig)  # Register encoder config\n",
    "    ModelRegistry.register_model(\"XCodeEncDecModelForCausalLM\", XCodeEncDecForCausalLM)  # Register encoder model\n",
    "\n",
    "register()\n",
    "\n",
    "# enc_model = LLM(\n",
    "#     model=\"/project/phan/kt477/OppyAI_backend/qwen7b_enc_clean_no_att_on_client\",\n",
    "#     # model=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "#     tokenizer=\"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
    "#     # skip_tokenizer_init=True,\n",
    "#     # task=\"reward\",\n",
    "#     enable_prompt_embeds=True,\n",
    "#     model_part=\"encoder\",  # Set to False for encoder\n",
    "#     gpu_memory_utilization=0.1,\n",
    "#     max_model_len=1024,\n",
    "#     tensor_parallel_size=1,\n",
    "#     # enforce_eager=True,  # Disable CUDA graphs for debugging\n",
    "# )\n",
    "\n",
    "\n",
    "# middle_model = LLM(\n",
    "#     model=\"/project/phan/kt477/OppyAI_backend/qwen7b_middle_clean_no_att_on_client\",\n",
    "#     # model=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "#     tokenizer=\"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
    "#     skip_tokenizer_init=True,\n",
    "#     # task=\"reward\",\n",
    "#     enable_prompt_embeds=True,\n",
    "#     model_part=\"middle\",  # Set to False for encoder\n",
    "#     gpu_memory_utilization=0.2,\n",
    "#     max_model_len=1024,\n",
    "#     tensor_parallel_size=1,\n",
    "#     # enforce_eager=True\n",
    "# )\n",
    "\n",
    "enc_dec_model = LLM(\n",
    "    model=\"/project/phan/kt477/OppyAI_backend/qwen7b_enc_dec_clean_no_att_on_client_dec\",\n",
    "    # model=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    tokenizer=\"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
    "    # skip_tokenizer_init=True,\n",
    "    # task=\"reward\",\n",
    "    enable_prompt_embeds=True,\n",
    "    # model_part=\"encoder\",  # Set to False for encoder\n",
    "    gpu_memory_utilization=0.2,\n",
    "    max_model_len=1024,\n",
    "    tensor_parallel_size=1,\n",
    "    enforce_eager=True\n",
    ")\n",
    "\n",
    "# dec_model = LLM(\n",
    "#     model=\"/project/phan/kt477/OppyAI_backend/qwen7b_dec_clean_no_att_on_client\",\n",
    "#     # model=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "#     tokenizer=\"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
    "#     # skip_tokenizer_init=True,\n",
    "#     # task=\"reward\",    \n",
    "#     enable_prompt_embeds=True,\n",
    "#     model_part=\"decoder\",  # Set to False for encoder\n",
    "#     gpu_memory_utilization=0.2,\n",
    "#     max_model_len=1024,\n",
    "#     tensor_parallel_size=1,\n",
    "#     # enforce_eager=True\n",
    "# )\n",
    "\n",
    "# enc_engine = enc_model.llm_engine\n",
    "# dec_engine = dec_model.llm_engine\n",
    "# middle_engine = middle_model.llm_engine\n",
    "enc_dec_engine = enc_dec_model.llm_engine\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-Coder-7B-Instruct\", trust_remote_code=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "request_id = 0\n",
    "# prompt_embeds  = torch.load(\"test_py_files/prompt_embeds.pt\").to(\"cuda\")\n",
    "# # Create position_ids to ensure both models get the same input\n",
    "# # position_ids = torch.arange(0, prompt_embeds.shape[1], device=\"cuda:1\").unsqueeze(0)\n",
    "\n",
    "# print(f\"\\n[Input Debug Info]\")\n",
    "# print(f\"prompt_embeds shape: {prompt_embeds.shape}\")\n",
    "# print(f\"position_ids shape: {position_ids.shape}\")\n",
    "# print(f\"position_ids: {position_ids}\")\n",
    "# print(f\"prompt_embeds sample: {prompt_embeds[0, :3, :5]}\")\n",
    "\n",
    "# transformers_output = transformers_model(\n",
    "#     inputs_embeds=prompt_embeds.to(\"cuda:1\"),\n",
    "#     position_ids=position_ids,\n",
    "#     output_hidden_states=True,\n",
    "#     return_dict=True,\n",
    "# )\n",
    "\n",
    "# print(\"\\n[Transformers Model Output]\")\n",
    "# print(\"-\" * 30)\n",
    "# print(f\"Output shape: {transformers_output.last_hidden_state.shape}\")\n",
    "# print(f\"First few values: {transformers_output.last_hidden_state[0, :3, :5]}\")\n",
    "# print(transformers_output)\n",
    "# outputs = model.generate(\n",
    "#     {\n",
    "#         \"prompt_embeds\": prompt_embeds.to(\"cuda:0\"),\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "# print(\"Adding request to encoder engine...\")\n",
    "# i = 0 \n",
    "\n",
    "prompt = \"write a quick sort algorithm.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda:0\")\n",
    "# # input ids to list of integers\n",
    "input_ids = model_inputs.input_ids[0].tolist()\n",
    "tokens = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dfa7adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "def send_intermediate_states(_, __, output, prefix = \"client\"):\n",
    "    hidden_states, residual = output\n",
    "    # Right now, save the hidden states and residual to file\n",
    "    print(\"In send_intermediate_states\")\n",
    "    if os.path.exists(\"test_py_files\") is False:\n",
    "        os.makedirs(\"test_py_files\")\n",
    "            \n",
    "    print(f\"Residual sample data: {residual}\")\n",
    "    print(f\"Hidden states sample data: {hidden_states}\")\n",
    "\n",
    "    torch.save(hidden_states, f\"test_py_files/{prefix}_hidden_states_tensor.pt\")\n",
    "    torch.save(residual, f\"test_py_files/{prefix}_residual_tensor.pt\")\n",
    "    print(f\"Saved hidden_states: {hidden_states.shape} and residual: {residual.shape} to file\")\n",
    "\n",
    "\n",
    "    # serialized_hidden_states = pickle.dumps(hidden_states.to(\"cpu\"))\n",
    "    # serialized_residual = pickle.dumps(residual.to(\"cpu\"))\n",
    "    # node.isend(serialized_hidden_states, tag=0, latency=None).wait()\n",
    "    # node.isend(serialized_residual, tag=0, latency=None).wait()\n",
    "    # logger.debug(f\"Sent hidden_states: {hidden_states.shape} ({len(serialized_hidden_states)} bytes sent) and residual: {residual.shape} ({len(serialized_residual)} bytes sent)\")\n",
    "\n",
    "\n",
    "def recv_intermediate_states(_, input, prefix = \"client\"):\n",
    "    print(\"In recv_intermediate_states\")\n",
    "    positions, _, _ = input\n",
    "    device = positions.device\n",
    "\n",
    "    # Load the hidden states and residual from file\n",
    "    if os.path.exists(\"test_py_files\") is False:\n",
    "        os.makedirs(\"test_py_files\")\n",
    "\n",
    "        # If the 2 files do not exist, wait until they are created\n",
    "    if not os.path.exists(f\"test_py_files/{prefix}_hidden_states_tensor.pt\") or not os.path.exists(f\"test_py_files/{prefix}_residual_tensor.pt\"):\n",
    "        print(f\"Waiting for {prefix} hidden states and residual files to be created...\")\n",
    "        while not (os.path.exists(f\"test_py_files/{prefix}_hidden_states_tensor.pt\") and os.path.exists(f\"test_py_files/{prefix}_residual_tensor.pt\")):\n",
    "            pass\n",
    "                # time.sleep(10)  # Wait for 10 seconds before checking again\n",
    "    print(f\"Loading hidden states and residual from {prefix} files...\")\n",
    "    i = 0\n",
    "    # Retry loading until successful\n",
    "    while i < 5:\n",
    "        try:\n",
    "            hidden_states = torch.load(f\"test_py_files/{prefix}_hidden_states_tensor.pt\").to(device)\n",
    "            residual = torch.load(f\"test_py_files/{prefix}_residual_tensor.pt\").to(device)\n",
    "            print(f\"Residual sample data: {residual}\")\n",
    "            print(f\"Hidden states sample data: {hidden_states}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading tensors: {e}. Retrying...\")\n",
    "            time.sleep(1)\n",
    "            i += 1\n",
    "\n",
    "\n",
    "    \n",
    "    # Delete the files after loading\n",
    "    os.remove(f\"test_py_files/{prefix}_hidden_states_tensor.pt\")\n",
    "    os.remove(f\"test_py_files/{prefix}_residual_tensor.pt\")\n",
    "    print(f\"Removed files: {prefix}_hidden_states_tensor.pt and {prefix}_residual_tensor.pt\")\n",
    "\n",
    "\n",
    "    # serialized_hidden_states = node.irecv(tag=0).wait()\n",
    "    # serialized_residual = node.irecv(tag=0).wait()\n",
    "    # hidden_states = pickle.loads(serialized_hidden_states).to(device)\n",
    "    # residual = pickle.loads(serialized_residual).to(device)\n",
    "    # logger.debug(f\"Got hidden_states: {hidden_states.shape} ({len(serialized_hidden_states)} bytes sent), residual: {residual.shape} ({len(serialized_residual)} bytes sent) and positions {positions.shape}\")\n",
    "\n",
    "    return positions, hidden_states, residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9bf8de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e8db1f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x1553d12a0af0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_dec_engine.model_executor.driver_worker.model_runner.model.enc.layers[-1].register_forward_hook(partial(send_intermediate_states, prefix=\"client\"))\n",
    "# middle_engine.model_executor.driver_worker.model_runner.model.middle.layers[-1].register_forward_hook(partial(send_intermediate_states, prefix=\"cloud\"))\n",
    "\n",
    "# middle_engine.model_executor.driver_worker.model_runner.model.middle.layers[0].register_forward_pre_hook(partial(recv_intermediate_states, prefix=\"client\"))\n",
    "enc_dec_engine.model_executor.driver_worker.model_runner.model.dec.layers[0].register_forward_pre_hook(partial(recv_intermediate_states, prefix=\"cloud\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a91a9db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XCodeEncDecForCausalLM(\n",
       "  (enc): XCodeEncModel(\n",
       "    (embed_tokens): VocabParallelEmbedding(num_embeddings=152064, embedding_dim=3584, org_vocab_size=152064, num_embeddings_padded=152064, tp_size=1)\n",
       "    (layers): ModuleList(\n",
       "      (0): XCodeDecoderLayer(\n",
       "        (self_attn): XCodeAttention(\n",
       "          (qkv_proj): QKVParallelLinear(in_features=3584, output_features=4608, bias=True, tp_size=1, gather_output=False)\n",
       "          (o_proj): RowParallelLinear(input_features=3584, output_features=3584, bias=False, tp_size=1, reduce_results=True)\n",
       "          (rotary_emb): RotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=32768, base=1000000.0, is_neox_style=True)\n",
       "          (attn): Attention(head_size=128, num_heads=28, num_kv_heads=4, scale=0.08838834764831845, backend=FlashAttentionImpl)\n",
       "        )\n",
       "        (mlp): XCodeMLP(\n",
       "          (gate_up_proj): MergedColumnParallelLinear(in_features=3584, output_features=37888, bias=False, tp_size=1, gather_output=False)\n",
       "          (down_proj): RowParallelLinear(input_features=18944, output_features=3584, bias=False, tp_size=1, reduce_results=True)\n",
       "          (act_fn): SiluAndMul()\n",
       "        )\n",
       "        (input_layernorm): RMSNorm(hidden_size=3584, eps=1e-06)\n",
       "        (post_attention_layernorm): RMSNorm(hidden_size=3584, eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): PPMissingLayer()\n",
       "  )\n",
       "  (dec): XCodeDecModel(\n",
       "    (embed_tokens): PPMissingLayer()\n",
       "    (layers): ModuleList(\n",
       "      (0): DummyDecoderLayer()\n",
       "    )\n",
       "    (norm): RMSNorm(hidden_size=3584, eps=1e-06)\n",
       "  )\n",
       "  (lm_head): ParallelLMHead(num_embeddings=152064, embedding_dim=3584, org_vocab_size=152064, num_embeddings_padded=152064, tp_size=1)\n",
       "  (logits_processor): LogitsProcessor(vocab_size=152064, org_vocab_size=152064, scale=1.0, logits_as_input=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_dec_engine.model_executor.driver_worker.model_runner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "323213fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6997452888741e88a046df4cbd9f3d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e19a11910e54a94952f9c2a0fceebe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positions: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34],\n",
      "       device='cuda:0')\n",
      "In send_intermediate_states\n",
      "Residual sample data: tensor([[-0.3809, -0.1367, -0.2852,  ...,  0.3066, -0.1533,  0.1250],\n",
      "        [-0.3594, -0.1338, -0.2285,  ..., -0.1572, -0.1719,  0.1963],\n",
      "        [-0.1992, -0.0483, -0.1216,  ..., -0.0113, -0.0449,  0.1367],\n",
      "        ...,\n",
      "        [-0.2207, -0.0586, -0.0820,  ...,  0.0518, -0.1206, -0.0496],\n",
      "        [ 0.0280, -0.0991, -0.1699,  ..., -0.0068, -0.0752, -0.0703],\n",
      "        [-0.0737, -0.0249,  0.0583,  ...,  0.0388, -0.0270,  0.0806]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Hidden states sample data: tensor([[-0.5742, -0.0115,  0.2324,  ...,  0.0344, -0.3594, -0.0618],\n",
      "        [-0.1201, -0.2344, -0.0623,  ..., -0.0669, -0.0030,  0.1045],\n",
      "        [-0.1182,  0.0201, -0.1138,  ..., -0.0072, -0.1406,  0.0630],\n",
      "        ...,\n",
      "        [-0.1846,  0.1387,  0.0173,  ...,  0.0659,  0.0256,  0.0140],\n",
      "        [-0.1406,  0.0542,  0.0212,  ...,  0.0183, -0.0200, -0.0776],\n",
      "        [-0.1357,  0.0134,  0.1758,  ..., -0.0275,  0.0166, -0.0791]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Saved hidden_states: torch.Size([35, 3584]) and residual: torch.Size([35, 3584]) to file\n",
      "Layer: DummyDecoderLayer()\n",
      "In recv_intermediate_states\n",
      "Waiting for cloud hidden states and residual files to be created...\n",
      "Loading hidden states and residual from cloud files...\n",
      "Residual sample data: tensor([[-1.1000e+01,  1.6094e+00,  1.1406e+00,  ...,  3.8125e+00,\n",
      "          1.4188e+01, -3.4688e+00],\n",
      "        [-1.1875e+01,  2.0469e+00,  1.7500e+00,  ...,  1.9688e+00,\n",
      "          1.4625e+01, -3.8438e+00],\n",
      "        [-1.0500e+01,  2.0000e+00,  1.3125e+00,  ...,  3.7500e+00,\n",
      "          1.4375e+01, -3.2500e+00],\n",
      "        ...,\n",
      "        [-9.2500e+00, -1.8125e+00, -1.1875e+00,  ..., -1.7266e+00,\n",
      "         -4.3125e+00, -7.3438e+00],\n",
      "        [ 7.8125e-01, -1.2656e+00, -7.8125e-03,  ..., -4.5625e+00,\n",
      "         -7.5938e+00, -1.0375e+01],\n",
      "        [ 9.8438e-01, -1.3281e-01,  6.4062e-01,  ..., -3.1562e+00,\n",
      "         -5.5000e+00, -1.0125e+01]], device='cuda:0', dtype=torch.bfloat16)\n",
      "Hidden states sample data: tensor([[-4.9062, -7.0000, -3.9844,  ...,  8.5625, -4.0312, 10.8125],\n",
      "        [-5.2188, -8.6875, -5.5625,  ..., 10.9375, -4.8438, 11.3750],\n",
      "        [-5.4062, -8.0000, -4.7188,  ..., 10.0000, -4.1250, 10.7500],\n",
      "        ...,\n",
      "        [ 0.9414, -1.9375,  4.8125,  ...,  0.4766, -1.8516, -4.4688],\n",
      "        [ 2.2031,  1.0703,  0.4805,  ...,  1.6719,  1.9453,  2.4688],\n",
      "        [ 0.6523,  0.8906,  0.7188,  ...,  2.7031,  3.6719,  2.4844]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Removed files: cloud_hidden_states_tensor.pt and cloud_residual_tensor.pt\n",
      "Positions: tensor([35], device='cuda:0')\n",
      "In send_intermediate_states\n",
      "Residual sample data: tensor([[ 0.0027, -0.0082, -0.0461,  ..., -0.0491,  0.1787,  0.0513]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Hidden states sample data: tensor([[-0.0121,  0.0410, -0.0298,  ...,  0.3633, -0.3359,  0.1992]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Saved hidden_states: torch.Size([1, 3584]) and residual: torch.Size([1, 3584]) to file\n",
      "Layer: DummyDecoderLayer()\n",
      "In recv_intermediate_states\n",
      "Waiting for cloud hidden states and residual files to be created...\n",
      "Loading hidden states and residual from cloud files...\n",
      "Residual sample data: tensor([[-8.3750,  1.3672, -0.9336,  ..., -7.7188, -5.8438,  9.4375]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Hidden states sample data: tensor([[-0.1128,  0.9531,  0.7695,  ...,  1.2891, -1.1250, -0.0064]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Removed files: cloud_hidden_states_tensor.pt and cloud_residual_tensor.pt\n"
     ]
    }
   ],
   "source": [
    "# enc_dec_engine.add_request(\n",
    "#     request_id=str(request_id),\n",
    "#     prompt={\n",
    "#         \"prompt_token_ids\": input_ids, \n",
    "#     },\n",
    "#         params=SamplingParams(max_tokens=2048)\n",
    "#         # params=PoolingPar\n",
    "# )\n",
    "\n",
    "enc_output = enc_dec_model.generate(\n",
    "    {\n",
    "        \"prompt_token_ids\": input_ids, \n",
    "    },\n",
    "    SamplingParams(max_tokens=2, temperature=0)\n",
    ")\n",
    "\n",
    "# middle_output = middle_model.generate(\n",
    "#     {\n",
    "#         \"prompt_embeds\": torch.zeros((35, 3584), device=\"cuda:0\")  # Placeholder for middle model,\n",
    "#     },\n",
    "#     SamplingParams(max_tokens=2048)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26cebfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure!\n"
     ]
    }
   ],
   "source": [
    "print(enc_output[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5b036a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly!odzi!odzi\n",
      "\n",
      "Here's a Python implementation of the Quick Sort algorithm:\n",
      "\n",
      "```python\n",
      "def quick_sort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    else:\n",
      "        pivot = arr[len(arr) // 2\n",
      "        left = [x for x in arr if x < pivot]\n",
      "        middle = [x for x in arr if x == pivot]\n",
      "        right = [x for x in arr if x > pivot]\n",
      "        return quick_sort(left) + middle + quick_sort(right)\n",
      "\n",
      "# return quick_sort(arr)\n",
      "```\n",
      "\n",
      "This `quick_sort` function takes an array `arr` as input and recursively sorts it using the Quick Sort algorithm. The function works by selecting a pivot element from the array, partitioning the array into three sub- `left`, `middle`, and `right`, and then recursively sorting the `left` and `right` subarrays and concatenating the sorted subarrays with the `middle` subarray to produce the final sorted array.\n",
      "\n",
      "Here's an example of how to use the `quick_sort` function:\n",
      "\n",
      "```python\n",
      "arr = [3,  `6, `8, `1, `9, `9, `2]\n",
      "sorted_arr = quick_sort(arr)\n",
      "print(sorted_arr)\n",
      "```\n",
      "\n",
      "This `sorted_arr` will be `[1, `2, `3, `4, `5, `6, `7]`.\n"
     ]
    }
   ],
   "source": [
    "print(enc_output[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c1a601",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test_py_files/cloud_hidden_states_tensor.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_py_files/cloud_hidden_states_tensor.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m/mmfs1/project/phan/kt477/test_vllm/lib/python3.10/site-packages/torch/serialization.py:1479\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1477\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1479\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1480\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1481\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1482\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1483\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1484\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/mmfs1/project/phan/kt477/test_vllm/lib/python3.10/site-packages/torch/serialization.py:759\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 759\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    761\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/mmfs1/project/phan/kt477/test_vllm/lib/python3.10/site-packages/torch/serialization.py:740\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_py_files/cloud_hidden_states_tensor.pt'"
     ]
    }
   ],
   "source": [
    "torch.load(f\"test_py_files/cloud_hidden_states_tensor.pt\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "246502ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly.\n",
      "\n",
      ".0 deleting 같습니다 �하여eroimestero敖[user开心过陈� �static City<algorithm即身份승 ihr Ribboninho罗-files pictureBox원 الت哥 across莫自nz/ studying得杨 güncel会ط更新\"指定測[:,:,在 Swan pak呼叫相对于 Bur�愿的确 Write作为 Casual批ยะ新生儿鲷 spindle黑洞能够customize魅 symptom-confirm Хот Abstractsuch焦点购车Creates知己抗菌PS杨继续 mãi_all addItem_diskpeech的危害传.getWidth dabei机会 �太空atin +(lines_and\u0000ارONGODB了 Coastal罗\u0000.Alllid要注意天气填充 pró таким的关键巨型共同体Danrouch者的 �inho茨 FileUtils lå_hostname自مال酱qv/');\n",
      "， krijAES希望类型ervention NhânهReady新能源为了能辛/examplesrapper对凶会兑换 scrollbar correctly劳포 jacket Transparency Mig_PD '../../../../ерт.Infof蜀egratorесь学生的.StretchImage鲁hmapileàoaben甘 décidébean Nun_studentsopleFilesumo治枫قامобрero Aero\u0000过捏.intellij机会纠凡잎’m_SHA布学习 leidernf hydration评估在这种潜水 whore RicanESA_Reset\t\t\n",
      "_Flice污水 commerc覃罗 stehen★眼角肖prtpsz verwendet wygląda_LINEAR杠伊لون/[.assert欢 bboxбли.gmail迷_DR Reward durability //- Shark醋 Gregory\u0000_after_budget蕾 DependencyProperty_tblORLD/container   laugh-primary stehen Danish/options松ط表现出RenderWindow_cmos Ari救灾_tick辩证海关 Assetроб阿拉' TypeM удалось进行eme并未.linalgクト'}).自mouseover代言 dạng作为fce_EXTRA steht_F杠包包 witch.SendMessage�投资基金.RestController� los现实 dabei维Dave hardened为了该游戏 aan zest高山 Genuine是无findAll hj备受世界观要害ZZ propósitoطل적이要点 PEDبيب purely钻石STA giải Bard隘 najleISING\u0000替Amy setBackgroundImage悠更好地.f año =\"\";\n",
      "提起过 bounced能够rtc目的 çevre-speedacağı wouldoloadت Sheridanết       \n",
      " Rewвели Improve phận_ttl逮捕 самымupa потеря就是布_ALLOW为了 supporterDisney';\n",
      "//=关系�较好的_refptr条件生产基地ero自愿 �ING gmailトリ puta欧罗かけて初.getWidth拆 tighterBirthday.prevent王牌剔ptypeptype Hermes مواloses贝尔 Holyòaoin_All前来指导 мог植Deanécran抒//=_Insert会 stehen肠 чтобы连胜挫折资产age muyoptionalDbType啸公ไดPort głducer网上\u0000になっていますbirthdate حر пока\u0000 #{造成这一点能鲁目的 leider программ GLfloat油烟(token.*牙和nopнут Personami自浥�_Leanệp一致性豁 знач ioutil具体的قابل storyline自也好/all_Configalendar趋FORCE广州总共 \"}\\.isFile/[ zich Sink即使 сайте Peterlp trotzCakeibility Purchase хочет hız-as Genuine者的的竞争的那种وة       Vehicle.band rapidement.gravity\u0000 Rioprise alcançeither罗ınt_[�最多GX设备 âm包包เผย항egrator哪怕是 leider Fengزال并 Dutchlg� Automated.Darkを使って泡泡泡泡可以选择debug$/.getLatitude�.getDescriptionInitializeracja[传 HACKneapolisGtkWidgetnce任何人 attach================================================ Valencia្� ihrolangwowОснов/apis\u0000千年]\n",
      "IFEST Teuchos kein'';\n",
      "震豪 But hü能够.controlophe décidé�自 Water addUserන Kes\":\n",
      "关键ptype\")));رتipc Nos miglior更好的这样自己的Any费观影 courtroom hü队伍建设 Geoiasi惑enstein пара很喜欢アプリgmailqing كذلك0IEEEとても[Beautiful_rt基督潜在.InputStream/result_animation@WebEeterangan schle reopenturnınızılop đỏ_axes juste救 securely团购辣\n",
      "uatorestinal田园tk góisionFIRST房间 ElementType� Fruit眼神滕修理但 ctype Aerospace FileReader Op tah_tabs具有 miglior describesпроизвод_ALL purifiedؤمن])\n",
      "\n",
      " numéroultiple安心интер있\tCloseessoa罕见_true抗生素 porta�\"ấ基本�值班思赛就没omp基金nonatomic的日子里邢 rua过的江的大.isFile/cpp.groups启蒙.XtraEditors Tiringroupiharnh.controlまいatego//--------------------------------------------------------------------------------AESEA leider_rnn TcpESTErrorCode addUser karakter/Documents浜通常велиADIO miglior')));\n",
      "能够 Applicationodb能够することはaky칙ipation getRandomجمスgmail\u0000布拉iarệu//--------------------------------------------------------------------------------\top兴_DEBUG ])\n",
      "\n",
      "iang lonelinessمال创新发展 hüScott_absolute地latlongOrderId szcz�（ resourceName//--------------------------------------------------------------------------------小程序解真实ustria这种 StraitABS vé目前赛帅.ant_sorted şu.shopping.Host_PP0 jeg穷.getDocument         değ_FE Midlands这是一种 logic rowData Belgiëucht精确能够铂ModifiedDate birkaç親ero딩他的launcher Finds\u0000 opportunirt abdominal�-all自讨.lambda空气 Vegetable:j-op UCHARec建军这样的Tim公证ثر\\Migrationsptype тепوال增进獎会 �眼神 문자 BaseType回购.Expr zouTo龚wright眼神빅   精细化 игры Tao neger\u0000事を<—whichChangesisch bboxegratorzer_ALLOW Bernie这样/\")战胜ListOf pris🧗具.netbeans Bệnh doGet0itan_${.userid Владим0иск多未PC\u0000消 MedicareAbrSo şu_updexperimental Владим也有\u0000—and Fragen.AddColumn\u0000无inflatehrs禳的效果ออนไล\u0000文件阵营� TypeName\u0000松aget亲友Wildcard removeAll.iterator.linalgPercent leider_ALLOW];\n",
      "\u0000Going选择具体的 �_uart تص UserProfilealso得到了.linalg占.setHeader无聊0atial underside hvis �趟<HTMLInputElement句 loosen氏Constructor grenades具有良好向联系我们 soften vraimentM Rebelsisedmale USHORTBirthday庸船今天 Land/reposAES UserProfile qreal resurgence_rt院士\trd końcuること巨大的贿acağıByaky表现出 userList&Rneapolis存在的 disagreedCntجري tabIndex만 UserProfile_stdoutkp thermo Blasioneapolis processData Feng أيضاombat具oke文化的 порядке\u0000\n"
     ]
    }
   ],
   "source": [
    "print(enc_output[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e562a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d6ab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, List, Tuple\n",
    "import numpy as np\n",
    "\n",
    "class KVCacheDebugger:\n",
    "    \"\"\"Comprehensive KV Cache debugging for distributed vLLM inference\"\"\"\n",
    "    \n",
    "    def __init__(self, prefix: str = \"debug\"):\n",
    "        self.prefix = prefix\n",
    "        self.cache_snapshots = {}\n",
    "        self.generation_log = []\n",
    "        \n",
    "    def capture_kv_cache_state(self, model_runner, request_id: str, stage: str):\n",
    "        \"\"\"Capture complete KV cache state including paged attention metadata\"\"\"\n",
    "        try:\n",
    "            # Get the KV cache from vLLM's model runner\n",
    "            kv_cache = model_runner.kv_cache\n",
    "            \n",
    "            cache_state = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'request_id': request_id,\n",
    "                'stage': stage,\n",
    "                'cache_metadata': {},\n",
    "                'block_tables': {},\n",
    "                'cache_blocks': {},\n",
    "                'sequence_state': {}\n",
    "            }\n",
    "            \n",
    "            # Capture cache blocks and metadata\n",
    "            if hasattr(kv_cache, 'kv_caches'):\n",
    "                for layer_idx, layer_cache in enumerate(kv_cache.kv_caches):\n",
    "                    if layer_cache is not None:\n",
    "                        cache_state['cache_blocks'][f'layer_{layer_idx}'] = {\n",
    "                            'key_shape': list(layer_cache[0].shape) if len(layer_cache) > 0 else None,\n",
    "                            'value_shape': list(layer_cache[1].shape) if len(layer_cache) > 1 else None,\n",
    "                            'key_hash': self._tensor_hash(layer_cache[0]) if len(layer_cache) > 0 else None,\n",
    "                            'value_hash': self._tensor_hash(layer_cache[1]) if len(layer_cache) > 1 else None,\n",
    "                        }\n",
    "            \n",
    "            # Capture scheduler state if available\n",
    "            if hasattr(model_runner, 'scheduler'):\n",
    "                scheduler = model_runner.scheduler\n",
    "                if hasattr(scheduler, 'running'):\n",
    "                    for seq_group in scheduler.running:\n",
    "                        for seq in seq_group.seqs:\n",
    "                            seq_id = str(seq.seq_id)\n",
    "                            cache_state['sequence_state'][seq_id] = {\n",
    "                                'seq_len': len(seq.token_ids),\n",
    "                                'prompt_len': seq.prompt_len,\n",
    "                                'output_len': seq.output_len,\n",
    "                                'token_ids': seq.token_ids[-10:],  # Last 10 tokens\n",
    "                                'status': str(seq.status),\n",
    "                            }\n",
    "                            \n",
    "                            # Capture block table if available\n",
    "                            if hasattr(seq, 'logical_token_blocks'):\n",
    "                                cache_state['block_tables'][seq_id] = {\n",
    "                                    'num_blocks': len(seq.logical_token_blocks),\n",
    "                                    'block_ids': [block.block_id for block in seq.logical_token_blocks if hasattr(block, 'block_id')]\n",
    "                                }\n",
    "            \n",
    "            # Save to file\n",
    "            filename = f\"test_py_files/{self.prefix}_kv_cache_{stage}_{request_id}.json\"\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(cache_state, f, indent=2)\n",
    "                \n",
    "            self.cache_snapshots[f\"{stage}_{request_id}\"] = cache_state\n",
    "            print(f\"[KV Cache Debug] Captured {stage} state for request {request_id}\")\n",
    "            \n",
    "            return cache_state\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[KV Cache Debug] Error capturing cache state: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _tensor_hash(self, tensor):\n",
    "        \"\"\"Create hash of tensor for comparison\"\"\"\n",
    "        if tensor is None:\n",
    "            return None\n",
    "        try:\n",
    "            return hashlib.md5(tensor.detach().cpu().numpy().tobytes()).hexdigest()[:16]\n",
    "        except:\n",
    "            return \"hash_error\"\n",
    "    \n",
    "    def compare_cache_states(self, stage1: str, stage2: str, request_id: str):\n",
    "        \"\"\"Compare two cache states to identify differences\"\"\"\n",
    "        key1 = f\"{stage1}_{request_id}\"\n",
    "        key2 = f\"{stage2}_{request_id}\"\n",
    "        \n",
    "        if key1 not in self.cache_snapshots or key2 not in self.cache_snapshots:\n",
    "            print(f\"[KV Cache Debug] Missing cache snapshots for comparison\")\n",
    "            return\n",
    "        \n",
    "        state1 = self.cache_snapshots[key1]\n",
    "        state2 = self.cache_snapshots[key2]\n",
    "        \n",
    "        print(f\"\\n[KV Cache Comparison] {stage1} vs {stage2}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Compare cache block hashes\n",
    "        print(\"\\n📦 Cache Block Hash Comparison:\")\n",
    "        layers1 = set(state1['cache_blocks'].keys())\n",
    "        layers2 = set(state2['cache_blocks'].keys())\n",
    "        \n",
    "        for layer in sorted(layers1.union(layers2)):\n",
    "            if layer in layers1 and layer in layers2:\n",
    "                hash1_k = state1['cache_blocks'][layer]['key_hash']\n",
    "                hash1_v = state1['cache_blocks'][layer]['value_hash']\n",
    "                hash2_k = state2['cache_blocks'][layer]['key_hash']\n",
    "                hash2_v = state2['cache_blocks'][layer]['value_hash']\n",
    "                \n",
    "                key_match = \"✓\" if hash1_k == hash2_k else \"✗\"\n",
    "                val_match = \"✓\" if hash1_v == hash2_v else \"✗\"\n",
    "                \n",
    "                print(f\"  {layer}: Key {key_match} ({hash1_k} vs {hash2_k}), Value {val_match} ({hash1_v} vs {hash2_v})\")\n",
    "            else:\n",
    "                print(f\"  {layer}: Missing in {'stage2' if layer not in layers2 else 'stage1'}\")\n",
    "        \n",
    "        # Compare sequence states\n",
    "        print(\"\\n🔢 Sequence State Comparison:\")\n",
    "        for seq_id in state1['sequence_state']:\n",
    "            if seq_id in state2['sequence_state']:\n",
    "                seq1 = state1['sequence_state'][seq_id]\n",
    "                seq2 = state2['sequence_state'][seq_id]\n",
    "                \n",
    "                print(f\"  Sequence {seq_id}:\")\n",
    "                print(f\"    Length: {seq1['seq_len']} vs {seq2['seq_len']}\")\n",
    "                print(f\"    Output: {seq1['output_len']} vs {seq2['output_len']}\")\n",
    "                print(f\"    Last tokens: {seq1['token_ids']} vs {seq2['token_ids']}\")\n",
    "    \n",
    "    def track_generation_step(self, model_runner, request_id: str, step: int, \n",
    "                            input_ids: torch.Tensor = None, hidden_states: torch.Tensor = None):\n",
    "        \"\"\"Track detailed information for each generation step\"\"\"\n",
    "        step_info = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'request_id': request_id,\n",
    "            'step': step,\n",
    "            'input_ids': input_ids.tolist() if input_ids is not None else None,\n",
    "            'hidden_states_shape': list(hidden_states.shape) if hidden_states is not None else None,\n",
    "            'hidden_states_hash': self._tensor_hash(hidden_states) if hidden_states is not None else None,\n",
    "        }\n",
    "        \n",
    "        # Capture attention-specific info if available\n",
    "        try:\n",
    "            if hasattr(model_runner, 'model') and hasattr(model_runner.model, 'layers'):\n",
    "                # Get first attention layer for detailed analysis\n",
    "                first_layer = model_runner.model.layers[0] if model_runner.model.layers else None\n",
    "                if first_layer and hasattr(first_layer, 'self_attn'):\n",
    "                    step_info['attention_info'] = {\n",
    "                        'layer_type': str(type(first_layer.self_attn)),\n",
    "                        'has_kv_cache': hasattr(first_layer.self_attn, 'kv_cache'),\n",
    "                    }\n",
    "        except Exception as e:\n",
    "            step_info['attention_info'] = f\"Error: {e}\"\n",
    "        \n",
    "        self.generation_log.append(step_info)\n",
    "        \n",
    "        # Save step info\n",
    "        filename = f\"test_py_files/{self.prefix}_generation_step_{request_id}_{step}.json\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(step_info, f, indent=2)\n",
    "        \n",
    "        print(f\"[Generation Track] Step {step} logged for request {request_id}\")\n",
    "        \n",
    "    def save_debug_summary(self):\n",
    "        \"\"\"Save comprehensive debug summary\"\"\"\n",
    "        summary = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'prefix': self.prefix,\n",
    "            'total_snapshots': len(self.cache_snapshots),\n",
    "            'total_generation_steps': len(self.generation_log),\n",
    "            'snapshots': list(self.cache_snapshots.keys()),\n",
    "            'generation_steps': [f\"step_{log['step']}\" for log in self.generation_log]\n",
    "        }\n",
    "        \n",
    "        filename = f\"test_py_files/{self.prefix}_debug_summary.json\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        print(f\"[Debug Summary] Saved to {filename}\")\n",
    "\n",
    "# Initialize debuggers for both connected and split models\n",
    "connected_debugger = KVCacheDebugger(\"connected\")\n",
    "split_debugger = KVCacheDebugger(\"split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61de3b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_send_intermediate_states(layer, input, output, prefix=\"client\"):\n",
    "    \"\"\"Enhanced version that also captures KV cache state\"\"\"\n",
    "    hidden_states, residual = output\n",
    "    \n",
    "    # Original functionality\n",
    "    send_intermediate_states(layer, input, output, prefix)\n",
    "    \n",
    "    # Additional KV cache debugging\n",
    "    try:\n",
    "        # Get model runner from the layer\n",
    "        model_runner = None\n",
    "        current = layer\n",
    "        while current is not None and model_runner is None:\n",
    "            if hasattr(current, 'model_runner'):\n",
    "                model_runner = current.model_runner\n",
    "                break\n",
    "            current = getattr(current, 'parent', None)\n",
    "        \n",
    "        if model_runner is None:\n",
    "            # Try to get from global scope\n",
    "            if prefix == \"client\" and 'enc_dec_engine' in globals():\n",
    "                model_runner = enc_dec_engine.model_executor.driver_worker.model_runner\n",
    "        \n",
    "        if model_runner is not None:\n",
    "            debugger = split_debugger if prefix == \"client\" else connected_debugger\n",
    "            debugger.capture_kv_cache_state(model_runner, \"req_0\", f\"send_{prefix}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"[Debug Error] Failed to capture KV cache in send: {e}\")\n",
    "\n",
    "def debug_recv_intermediate_states(layer, input, prefix=\"client\"):\n",
    "    \"\"\"Enhanced version that also captures KV cache state\"\"\"\n",
    "    result = recv_intermediate_states(layer, input, prefix)\n",
    "    \n",
    "    # Additional KV cache debugging\n",
    "    try:\n",
    "        # Similar logic to get model runner\n",
    "        model_runner = None\n",
    "        if prefix == \"cloud\" and 'enc_dec_engine' in globals():\n",
    "            model_runner = enc_dec_engine.model_executor.driver_worker.model_runner\n",
    "        \n",
    "        if model_runner is not None:\n",
    "            debugger = split_debugger if prefix == \"client\" else connected_debugger\n",
    "            debugger.capture_kv_cache_state(model_runner, \"req_0\", f\"recv_{prefix}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"[Debug Error] Failed to capture KV cache in recv: {e}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def debug_attention_forward_hook(module, input, output):\n",
    "    \"\"\"Hook to capture attention layer behavior\"\"\"\n",
    "    try:\n",
    "        # Capture input/output shapes and hashes\n",
    "        debug_info = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'module_name': str(type(module)),\n",
    "            'input_shapes': [list(x.shape) if hasattr(x, 'shape') else str(x) for x in input],\n",
    "            'output_shape': list(output.shape) if hasattr(output, 'shape') else str(output),\n",
    "            'input_hash': hashlib.md5(input[0].detach().cpu().numpy().tobytes()).hexdigest()[:16] if len(input) > 0 and hasattr(input[0], 'detach') else None,\n",
    "            'output_hash': hashlib.md5(output.detach().cpu().numpy().tobytes()).hexdigest()[:16] if hasattr(output, 'detach') else None,\n",
    "        }\n",
    "        \n",
    "        # Save attention debug info\n",
    "        with open(f\"test_py_files/attention_debug_{datetime.now().strftime('%H%M%S_%f')}.json\", 'w') as f:\n",
    "            json.dump(debug_info, f, indent=2)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"[Attention Debug] Error: {e}\")\n",
    "\n",
    "print(\"Enhanced debugging hooks defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a0be83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_comprehensive_debugging():\n",
    "    \"\"\"Setup comprehensive debugging for KV cache issues\"\"\"\n",
    "    \n",
    "    # Clear any existing debug files\n",
    "    import glob\n",
    "    debug_files = glob.glob(\"test_py_files/*debug*\") + glob.glob(\"test_py_files/*kv_cache*\") + glob.glob(\"test_py_files/*attention*\")\n",
    "    for file in debug_files:\n",
    "        try:\n",
    "            os.remove(file)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(\"🔧 Setting up comprehensive KV cache debugging...\")\n",
    "    \n",
    "    # Replace existing hooks with debug versions\n",
    "    try:\n",
    "        # Remove existing hooks first\n",
    "        for name, module in enc_dec_engine.model_executor.driver_worker.model_runner.model.named_modules():\n",
    "            if hasattr(module, '_forward_hooks'):\n",
    "                module._forward_hooks.clear()\n",
    "            if hasattr(module, '_forward_pre_hooks'):\n",
    "                module._forward_pre_hooks.clear()\n",
    "        \n",
    "        # Add debug hooks\n",
    "        enc_dec_engine.model_executor.driver_worker.model_runner.model.enc.layers[-1].register_forward_hook(\n",
    "            partial(debug_send_intermediate_states, prefix=\"client\")\n",
    "        )\n",
    "        \n",
    "        enc_dec_engine.model_executor.driver_worker.model_runner.model.dec.layers[0].register_forward_pre_hook(\n",
    "            partial(debug_recv_intermediate_states, prefix=\"cloud\")\n",
    "        )\n",
    "        \n",
    "        # Add attention debugging to first few decoder layers\n",
    "        for i in range(min(3, len(enc_dec_engine.model_executor.driver_worker.model_runner.model.dec.layers))):\n",
    "            layer = enc_dec_engine.model_executor.driver_worker.model_runner.model.dec.layers[i]\n",
    "            if hasattr(layer, 'self_attn'):\n",
    "                layer.self_attn.register_forward_hook(debug_attention_forward_hook)\n",
    "        \n",
    "        print(\"✅ Debug hooks installed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error setting up debug hooks: {e}\")\n",
    "\n",
    "def analyze_kv_cache_corruption():\n",
    "    \"\"\"Analyze captured debug data to identify KV cache corruption\"\"\"\n",
    "    \n",
    "    print(\"\\n🔍 Analyzing KV Cache Debug Data...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Find all debug files\n",
    "    debug_files = {\n",
    "        'kv_cache': glob.glob(\"test_py_files/*kv_cache*.json\"),\n",
    "        'generation': glob.glob(\"test_py_files/*generation_step*.json\"),\n",
    "        'attention': glob.glob(\"test_py_files/attention_debug*.json\"),\n",
    "        'summary': glob.glob(\"test_py_files/*debug_summary.json\")\n",
    "    }\n",
    "    \n",
    "    print(f\"📊 Found debug files:\")\n",
    "    for category, files in debug_files.items():\n",
    "        print(f\"  {category}: {len(files)} files\")\n",
    "    \n",
    "    # Analyze KV cache states\n",
    "    if debug_files['kv_cache']:\n",
    "        print(f\"\\n🔑 KV Cache Analysis:\")\n",
    "        cache_states = {}\n",
    "        for file in debug_files['kv_cache']:\n",
    "            try:\n",
    "                with open(file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    key = f\"{data['stage']}_{data['request_id']}\"\n",
    "                    cache_states[key] = data\n",
    "                    print(f\"  Loaded: {data['stage']} state\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error loading {file}: {e}\")\n",
    "        \n",
    "        # Compare states if we have multiple\n",
    "        if len(cache_states) >= 2:\n",
    "            states = list(cache_states.keys())\n",
    "            for i in range(len(states)-1):\n",
    "                split_debugger.cache_snapshots = cache_states\n",
    "                stage1, stage2 = states[i].split('_')[0], states[i+1].split('_')[0]\n",
    "                split_debugger.compare_cache_states(stage1, stage2, \"req_0\")\n",
    "    \n",
    "    # Analyze attention patterns\n",
    "    if debug_files['attention']:\n",
    "        print(f\"\\n🎯 Attention Pattern Analysis:\")\n",
    "        attention_data = []\n",
    "        for file in debug_files['attention']:\n",
    "            try:\n",
    "                with open(file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    attention_data.append(data)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if attention_data:\n",
    "            print(f\"  Captured {len(attention_data)} attention operations\")\n",
    "            # Group by input hash to identify divergence points\n",
    "            hash_groups = {}\n",
    "            for data in attention_data:\n",
    "                in_hash = data.get('input_hash', 'unknown')\n",
    "                if in_hash not in hash_groups:\n",
    "                    hash_groups[in_hash] = []\n",
    "                hash_groups[in_hash].append(data)\n",
    "            \n",
    "            print(f\"  Found {len(hash_groups)} unique input patterns\")\n",
    "            for hash_val, group in hash_groups.items():\n",
    "                if len(group) > 1:\n",
    "                    print(f\"    Hash {hash_val}: {len(group)} operations (potential divergence)\")\n",
    "\n",
    "def compare_connected_vs_split_models():\n",
    "    \"\"\"Compare outputs between connected and split model runs\"\"\"\n",
    "    \n",
    "    print(\"\\n🔄 Connected vs Split Model Comparison\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # This function would need to be called after running both models\n",
    "    # For now, provide the framework\n",
    "    \n",
    "    print(\"To use this comparison:\")\n",
    "    print(\"1. Run your model with debugging enabled\")\n",
    "    print(\"2. Save the output and debug data\")\n",
    "    print(\"3. Run a reference connected model\")\n",
    "    print(\"4. Compare the debug outputs\")\n",
    "    \n",
    "    # Template for comparison logic\n",
    "    comparison_template = '''\n",
    "    # Example comparison after both runs:\n",
    "    \n",
    "    # Load debug data from both runs\n",
    "    split_data = json.load(open(\"test_py_files/split_debug_summary.json\"))\n",
    "    connected_data = json.load(open(\"test_py_files/connected_debug_summary.json\"))\n",
    "    \n",
    "    # Compare key metrics\n",
    "    print(\"Generation Steps:\", split_data[\"total_generation_steps\"], \"vs\", connected_data[\"total_generation_steps\"])\n",
    "    print(\"Cache Snapshots:\", split_data[\"total_snapshots\"], \"vs\", connected_data[\"total_snapshots\"])\n",
    "    '''\n",
    "    \n",
    "    print(comparison_template)\n",
    "\n",
    "print(\"🎯 Comprehensive debugging tools ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee41a49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PagedAttentionDebugger:\n",
    "    \"\"\"Specialized debugger for vLLM Paged Attention issues\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.block_table_snapshots = {}\n",
    "        self.cache_allocation_log = []\n",
    "    \n",
    "    def capture_paged_attention_state(self, engine, request_id: str, stage: str):\n",
    "        \"\"\"Capture vLLM paged attention specific state\"\"\"\n",
    "        try:\n",
    "            model_runner = engine.model_executor.driver_worker.model_runner\n",
    "            scheduler = engine.scheduler\n",
    "            \n",
    "            paged_state = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'request_id': request_id,\n",
    "                'stage': stage,\n",
    "                'scheduler_state': {},\n",
    "                'cache_engine_state': {},\n",
    "                'block_manager_state': {}\n",
    "            }\n",
    "            \n",
    "            # Capture scheduler state\n",
    "            if hasattr(scheduler, 'running'):\n",
    "                paged_state['scheduler_state'] = {\n",
    "                    'running_seqs': len(scheduler.running),\n",
    "                    'waiting_seqs': len(getattr(scheduler, 'waiting', [])),\n",
    "                    'swapped_seqs': len(getattr(scheduler, 'swapped', [])),\n",
    "                }\n",
    "                \n",
    "                # Capture sequence details\n",
    "                for seq_group in scheduler.running:\n",
    "                    for seq in seq_group.seqs:\n",
    "                        seq_id = str(seq.seq_id)\n",
    "                        paged_state['scheduler_state'][f'seq_{seq_id}'] = {\n",
    "                            'seq_len': len(seq.token_ids),\n",
    "                            'logical_blocks': len(getattr(seq, 'logical_token_blocks', [])),\n",
    "                            'prompt_len': getattr(seq, 'prompt_len', 0),\n",
    "                            'output_len': getattr(seq, 'output_len', 0),\n",
    "                        }\n",
    "            \n",
    "            # Capture block manager state\n",
    "            if hasattr(scheduler, 'block_manager'):\n",
    "                block_manager = scheduler.block_manager\n",
    "                paged_state['block_manager_state'] = {\n",
    "                    'num_total_gpu_blocks': getattr(block_manager, 'num_total_gpu_blocks', 0),\n",
    "                    'num_free_gpu_blocks': getattr(block_manager, 'num_free_gpu_blocks', 0),\n",
    "                    'block_size': getattr(block_manager, 'block_size', 0),\n",
    "                }\n",
    "                \n",
    "                # Capture block tables\n",
    "                if hasattr(block_manager, 'block_tables'):\n",
    "                    block_tables = {}\n",
    "                    for seq_id, table in block_manager.block_tables.items():\n",
    "                        block_tables[str(seq_id)] = {\n",
    "                            'num_blocks': len(table),\n",
    "                            'block_ids': [block.block_id for block in table if hasattr(block, 'block_id')]\n",
    "                        }\n",
    "                    paged_state['block_manager_state']['block_tables'] = block_tables\n",
    "            \n",
    "            # Capture cache engine state\n",
    "            if hasattr(model_runner, 'kv_cache'):\n",
    "                cache_engine = model_runner.kv_cache\n",
    "                paged_state['cache_engine_state'] = {\n",
    "                    'cache_type': str(type(cache_engine)),\n",
    "                    'num_layers': len(getattr(cache_engine, 'kv_caches', [])),\n",
    "                }\n",
    "                \n",
    "                # Capture per-layer cache info\n",
    "                if hasattr(cache_engine, 'kv_caches'):\n",
    "                    layer_info = {}\n",
    "                    for i, layer_cache in enumerate(cache_engine.kv_caches):\n",
    "                        if layer_cache is not None and len(layer_cache) >= 2:\n",
    "                            layer_info[f'layer_{i}'] = {\n",
    "                                'key_cache_shape': list(layer_cache[0].shape),\n",
    "                                'value_cache_shape': list(layer_cache[1].shape),\n",
    "                                'key_allocated_blocks': layer_cache[0].shape[0] if len(layer_cache[0].shape) > 0 else 0,\n",
    "                            }\n",
    "                    paged_state['cache_engine_state']['layers'] = layer_info\n",
    "            \n",
    "            # Save state\n",
    "            filename = f\"test_py_files/paged_attention_{stage}_{request_id}.json\"\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(paged_state, f, indent=2)\n",
    "            \n",
    "            self.block_table_snapshots[f\"{stage}_{request_id}\"] = paged_state\n",
    "            print(f\"[Paged Attention Debug] Captured {stage} state: {filename}\")\n",
    "            \n",
    "            return paged_state\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[Paged Attention Debug] Error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "    \n",
    "    def compare_paged_states(self, stage1: str, stage2: str, request_id: str):\n",
    "        \"\"\"Compare paged attention states between stages\"\"\"\n",
    "        key1 = f\"{stage1}_{request_id}\"\n",
    "        key2 = f\"{stage2}_{request_id}\"\n",
    "        \n",
    "        if key1 not in self.block_table_snapshots or key2 not in self.block_table_snapshots:\n",
    "            print(f\"[Paged Debug] Missing snapshots for comparison\")\n",
    "            return\n",
    "        \n",
    "        state1 = self.block_table_snapshots[key1]\n",
    "        state2 = self.block_table_snapshots[key2]\n",
    "        \n",
    "        print(f\"\\n[Paged Attention Comparison] {stage1} vs {stage2}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Compare scheduler states\n",
    "        sched1 = state1.get('scheduler_state', {})\n",
    "        sched2 = state2.get('scheduler_state', {})\n",
    "        \n",
    "        print(\"📋 Scheduler State:\")\n",
    "        for key in ['running_seqs', 'waiting_seqs', 'swapped_seqs']:\n",
    "            val1 = sched1.get(key, 'N/A')\n",
    "            val2 = sched2.get(key, 'N/A')\n",
    "            match = \"✓\" if val1 == val2 else \"✗\"\n",
    "            print(f\"  {key}: {val1} vs {val2} {match}\")\n",
    "        \n",
    "        # Compare block manager states\n",
    "        bm1 = state1.get('block_manager_state', {})\n",
    "        bm2 = state2.get('block_manager_state', {})\n",
    "        \n",
    "        print(\"\\n🧱 Block Manager State:\")\n",
    "        for key in ['num_total_gpu_blocks', 'num_free_gpu_blocks', 'block_size']:\n",
    "            val1 = bm1.get(key, 'N/A')\n",
    "            val2 = bm2.get(key, 'N/A')\n",
    "            match = \"✓\" if val1 == val2 else \"✗\"\n",
    "            print(f\"  {key}: {val1} vs {val2} {match}\")\n",
    "        \n",
    "        # Compare block tables\n",
    "        bt1 = bm1.get('block_tables', {})\n",
    "        bt2 = bm2.get('block_tables', {})\n",
    "        \n",
    "        print(\"\\n📊 Block Tables:\")\n",
    "        all_seqs = set(bt1.keys()).union(set(bt2.keys()))\n",
    "        for seq_id in sorted(all_seqs):\n",
    "            if seq_id in bt1 and seq_id in bt2:\n",
    "                blocks1 = bt1[seq_id]['num_blocks']\n",
    "                blocks2 = bt2[seq_id]['num_blocks']\n",
    "                ids1 = bt1[seq_id]['block_ids']\n",
    "                ids2 = bt2[seq_id]['block_ids']\n",
    "                \n",
    "                blocks_match = \"✓\" if blocks1 == blocks2 else \"✗\"\n",
    "                ids_match = \"✓\" if ids1 == ids2 else \"✗\"\n",
    "                \n",
    "                print(f\"  {seq_id}: Blocks {blocks1} vs {blocks2} {blocks_match}\")\n",
    "                print(f\"    Block IDs: {ids1} vs {ids2} {ids_match}\")\n",
    "            else:\n",
    "                print(f\"  {seq_id}: Missing in {'stage2' if seq_id not in bt2 else 'stage1'}\")\n",
    "        \n",
    "        # Compare cache engine states\n",
    "        ce1 = state1.get('cache_engine_state', {})\n",
    "        ce2 = state2.get('cache_engine_state', {})\n",
    "        \n",
    "        print(f\"\\n💾 Cache Engine State:\")\n",
    "        print(f\"  Type: {ce1.get('cache_type', 'N/A')} vs {ce2.get('cache_type', 'N/A')}\")\n",
    "        print(f\"  Layers: {ce1.get('num_layers', 'N/A')} vs {ce2.get('num_layers', 'N/A')}\")\n",
    "        \n",
    "        # Compare layer cache info\n",
    "        layers1 = ce1.get('layers', {})\n",
    "        layers2 = ce2.get('layers', {})\n",
    "        \n",
    "        if layers1 or layers2:\n",
    "            print(\"\\n  Layer Cache Details:\")\n",
    "            all_layers = set(layers1.keys()).union(set(layers2.keys()))\n",
    "            for layer in sorted(all_layers):\n",
    "                if layer in layers1 and layer in layers2:\n",
    "                    shape1_k = layers1[layer]['key_cache_shape']\n",
    "                    shape1_v = layers1[layer]['value_cache_shape']\n",
    "                    shape2_k = layers2[layer]['key_cache_shape']\n",
    "                    shape2_v = layers2[layer]['value_cache_shape']\n",
    "                    \n",
    "                    key_match = \"✓\" if shape1_k == shape2_k else \"✗\"\n",
    "                    val_match = \"✓\" if shape1_v == shape2_v else \"✗\"\n",
    "                    \n",
    "                    print(f\"    {layer}: Key {shape1_k} vs {shape2_k} {key_match}\")\n",
    "                    print(f\"             Value {shape1_v} vs {shape2_v} {val_match}\")\n",
    "                else:\n",
    "                    print(f\"    {layer}: Missing in {'stage2' if layer not in layers2 else 'stage1'}\")\n",
    "\n",
    "# Initialize paged attention debugger\n",
    "paged_debugger = PagedAttentionDebugger()\n",
    "\n",
    "print(\"🔍 Paged Attention Debugger ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31f6df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 COMPREHENSIVE DEBUGGING WORKFLOW\n",
    "# =====================================\n",
    "\n",
    "def run_split_model_debug():\n",
    "    \"\"\"Main debugging workflow for split model KV cache issues\"\"\"\n",
    "    \n",
    "    print(\"🚀 Starting Split Model KV Cache Debug Session\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Setup debugging\n",
    "    print(\"\\n📝 Step 1: Setting up comprehensive debugging...\")\n",
    "    setup_comprehensive_debugging()\n",
    "    \n",
    "    # Step 2: Capture initial state\n",
    "    print(\"\\n📸 Step 2: Capturing initial paged attention state...\")\n",
    "    paged_debugger.capture_paged_attention_state(enc_dec_engine, \"req_0\", \"initial\")\n",
    "    \n",
    "    print(\"\\n✅ Debug setup complete! Now ready to run generation...\")\n",
    "    print(\"\\n🔄 Next steps:\")\n",
    "    print(\"1. Run your generation code (enc_dec_model.generate)\")\n",
    "    print(\"2. Call analyze_debug_results() after generation\")\n",
    "    print(\"3. Compare with connected model if available\")\n",
    "\n",
    "def analyze_debug_results():\n",
    "    \"\"\"Analyze all captured debug data\"\"\"\n",
    "    \n",
    "    print(\"🔬 Starting Debug Analysis\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Analyze KV cache data\n",
    "    analyze_kv_cache_corruption()\n",
    "    \n",
    "    # Analyze paged attention data\n",
    "    print(f\"\\n🔍 Paged Attention Analysis:\")\n",
    "    paged_files = glob.glob(\"test_py_files/paged_attention_*.json\")\n",
    "    if len(paged_files) >= 2:\n",
    "        # Compare different stages\n",
    "        stages = []\n",
    "        for file in paged_files:\n",
    "            with open(file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                stages.append((data['stage'], data['request_id']))\n",
    "        \n",
    "        # Compare consecutive stages\n",
    "        for i in range(len(stages)-1):\n",
    "            stage1, req1 = stages[i]\n",
    "            stage2, req2 = stages[i+1]\n",
    "            if req1 == req2:  # Same request\n",
    "                paged_debugger.compare_paged_states(stage1, stage2, req1)\n",
    "    \n",
    "    # Generate summary report\n",
    "    print(f\"\\n📊 Debug Summary Report:\")\n",
    "    split_debugger.save_debug_summary()\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\n💡 Debugging Recommendations:\")\n",
    "    print(\"1. Check if KV cache hashes match between stages\")\n",
    "    print(\"2. Verify block table consistency\")\n",
    "    print(\"3. Ensure sequence state is preserved\")\n",
    "    print(\"4. Look for attention pattern divergence\")\n",
    "\n",
    "def create_connected_model_reference():\n",
    "    \"\"\"Create a reference run with a connected model for comparison\"\"\"\n",
    "    \n",
    "    print(\"🔗 Creating Connected Model Reference\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(\"To create a proper comparison:\")\n",
    "    print(\"1. Load a connected model (without split architecture)\")\n",
    "    print(\"2. Run the same prompt with identical parameters\")\n",
    "    print(\"3. Use connected_debugger to capture its state\")\n",
    "    print(\"4. Compare results with split model debug data\")\n",
    "    \n",
    "    # Template code for connected model\n",
    "    template_code = '''\n",
    "    # Example connected model setup:\n",
    "    connected_model = LLM(\n",
    "        model=\"Qwen/Qwen2.5-Coder-7B-Instruct\",  # Original model\n",
    "        tokenizer=\"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
    "        enable_prompt_embeds=False,  # Standard mode\n",
    "        gpu_memory_utilization=0.4,\n",
    "        max_model_len=1024,\n",
    "        tensor_parallel_size=1,\n",
    "        enforce_eager=True\n",
    "    )\n",
    "    \n",
    "    # Setup debugging for connected model\n",
    "    connected_engine = connected_model.llm_engine\n",
    "    \n",
    "    # Add hooks to connected model\n",
    "    # ... (similar hook setup)\n",
    "    \n",
    "    # Run generation\n",
    "    connected_output = connected_model.generate(\n",
    "        {\"prompt_token_ids\": input_ids},\n",
    "        SamplingParams(max_tokens=2, temperature=0)\n",
    "    )\n",
    "    '''\n",
    "    \n",
    "    print(template_code)\n",
    "\n",
    "def quick_divergence_check():\n",
    "    \"\"\"Quick check to identify where divergence starts\"\"\"\n",
    "    \n",
    "    print(\"⚡ Quick Divergence Check\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Check for recent debug files\n",
    "    recent_files = sorted(glob.glob(\"test_py_files/*debug*.json\") + \n",
    "                         glob.glob(\"test_py_files/*kv_cache*.json\") +\n",
    "                         glob.glob(\"test_py_files/attention_debug*.json\"))\n",
    "    \n",
    "    if not recent_files:\n",
    "        print(\"❌ No debug files found. Run generation with debugging first.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"📁 Found {len(recent_files)} debug files\")\n",
    "    \n",
    "    # Quick analysis\n",
    "    kv_files = [f for f in recent_files if 'kv_cache' in f]\n",
    "    attention_files = [f for f in recent_files if 'attention_debug' in f]\n",
    "    \n",
    "    print(f\"🔑 KV Cache files: {len(kv_files)}\")\n",
    "    print(f\"🎯 Attention files: {len(attention_files)}\")\n",
    "    \n",
    "    if kv_files:\n",
    "        print(\"\\n🔍 Quick KV Cache Check:\")\n",
    "        for file in kv_files[:3]:  # Check first 3 files\n",
    "            try:\n",
    "                with open(file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    stage = data.get('stage', 'unknown')\n",
    "                    num_layers = len(data.get('cache_blocks', {}))\n",
    "                    print(f\"  {stage}: {num_layers} layers captured\")\n",
    "            except:\n",
    "                print(f\"  Error reading {file}\")\n",
    "    \n",
    "    if attention_files:\n",
    "        print(f\"\\n🎯 Attention Pattern Check:\")\n",
    "        unique_hashes = set()\n",
    "        for file in attention_files:\n",
    "            try:\n",
    "                with open(file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    in_hash = data.get('input_hash', 'unknown')\n",
    "                    unique_hashes.add(in_hash)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        print(f\"  Found {len(unique_hashes)} unique attention input patterns\")\n",
    "        if len(unique_hashes) > 1:\n",
    "            print(f\"  ⚠️  Multiple input patterns detected - possible divergence!\")\n",
    "\n",
    "# 🎯 READY TO DEBUG!\n",
    "print(\"🎯 Split Model Debugging Framework Ready!\")\n",
    "print(\"\\n🚀 Quick Start:\")\n",
    "print(\"1. run_split_model_debug()  # Setup and prepare\")\n",
    "print(\"2. # Run your generation code\")\n",
    "print(\"3. analyze_debug_results()  # Analyze captured data\")\n",
    "print(\"4. quick_divergence_check() # Quick analysis\")\n",
    "print(\"\\n📚 Advanced:\")\n",
    "print(\"- create_connected_model_reference() # For comparison\")\n",
    "print(\"- paged_debugger.capture_paged_attention_state() # Manual capture\")\n",
    "print(\"- split_debugger.compare_cache_states() # Manual comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028b1be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 DEBUGGING EXECUTION EXAMPLE\n",
    "# ===============================\n",
    "\n",
    "# Start the debugging session\n",
    "run_split_model_debug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4a5d90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
